{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f3b14cf6-a36f-48a4-994c-f37d0a8dea41",
      "metadata": {
        "id": "f3b14cf6-a36f-48a4-994c-f37d0a8dea41"
      },
      "source": [
        "# XAI Classification of Lateral Spreading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69268af7",
      "metadata": {
        "id": "69268af7"
      },
      "source": [
        "**Exercise:** \n",
        "[![Try on DesignSafe](https://raw.githubusercontent.com/DesignSafe-Training/xai/main/DesignSafe-Badge.svg)](https://jupyter.designsafe-ci.org/hub/user-redirect/lab/tree/CommunityData/Training/xai/01-classification-exercise.ipynb)\n",
        "\n",
        "**Solution:** [![Try on DesignSafe](https://raw.githubusercontent.com/DesignSafe-Training/xai/main/DesignSafe-Badge.svg)](https://jupyter.designsafe-ci.org/hub/user-redirect/lab/tree/CommunityData/Training/xai/01-classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f2e500",
      "metadata": {
        "id": "40f2e500"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f25036",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7f25036",
        "outputId": "7196f83f-b0f9-4706-e2cb-68bc53756e73"
      },
      "outputs": [],
      "source": [
        "!pip3 install scikit-learn pandas --quiet\n",
        "!pip3 install xgboost --quiet\n",
        "!pip3 install shap --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b33fa6",
      "metadata": {
        "id": "86b33fa6"
      },
      "source": [
        "## Liquefaction\n",
        "\n",
        "> Durante, M. G., & Rathje, E. M. (2021). An exploration of the use of machine learning to predict lateral spreading. Earthquake Spectra, 37(4), 2288-2314.\n",
        "\n",
        "Soil liquefaction is a phenomenon that typically occurs in saturated loose sandy soils subjected to rapid loading conditions, such as earthquakes. The generation of excess pore water pressure is a direct consequence of the rapid loading, which can lead to a sudden reduction in the strength and stiffness of the soil. In the presence of gently sloping ground or near the free face of a slope, the occurrence of earthquake-induced liquefaction may generate lateral displacements, known as lateral spreading."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ee4084",
      "metadata": {
        "id": "f7ee4084"
      },
      "source": [
        "![liquefaction](liquefaction.png)\n",
        "\n",
        "> Fig: (a) Observed liquefaction-related damage (data from NZGD, 2013), and (b) lateral spreading\n",
        "horizontal displacement observed from optical image correlation (data from Rathje et al., 2017b) in the\n",
        "Avon River area for the 2011 Christchurch earthquake"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7928a657",
      "metadata": {
        "id": "7928a657"
      },
      "source": [
        "## Lateral spreading classification\n",
        "\n",
        "Durante and Rathje (2021) classified sites that experienced more than 0.3 m displacement as lateral spreading. We now evaluate different factors that influence soil lateral spreading, such as (i) Ground Water Table (ground water closer to surface means more chance of liquefaction), (ii) slope angle (steeper the slope more lateral spreading), (iii) PGA - Peak Ground acceleration (intensity of earthquake shaking), and (iv) elevation (certain sites on high terrace don't show lateral spreading)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589d7c98",
      "metadata": {
        "id": "589d7c98"
      },
      "source": [
        "![Factors affecting liquefaction](liq-factors.png)\n",
        "\n",
        "> Image credits: Durante and Rathje (2021)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1600baa1-0531-49db-8130-3142f4a1f87c",
      "metadata": {
        "id": "1600baa1-0531-49db-8130-3142f4a1f87c"
      },
      "source": [
        "### Explore data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66794fe1",
      "metadata": {
        "id": "66794fe1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac9b734",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read data from DesignSafe community data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4cb8d4-08cd-439f-aaf1-99dce84733ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fe4cb8d4-08cd-439f-aaf1-99dce84733ad",
        "outputId": "7ee73a6e-be41-48f3-ad2b-0bc8c85d1cfa"
      },
      "outputs": [],
      "source": [
        "# Explore your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9200619d",
      "metadata": {
        "id": "9200619d"
      },
      "source": [
        "### Filtering for features\n",
        "\n",
        "Remove any feature in the dataset that we don't want to include in the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8add03-5ff0-4fde-ac6e-676ae9414e86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ad8add03-5ff0-4fde-ac6e-676ae9414e86",
        "outputId": "55b0ba72-2249-48a7-a722-5cb6603e1f81"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['Test ID', 'Elevation'], axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259f8a61-2a28-4a9d-97c3-eaeb5104c676",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "259f8a61-2a28-4a9d-97c3-eaeb5104c676",
        "outputId": "a5737803-c6f8-4bb9-b875-83c424be4d4c"
      },
      "outputs": [],
      "source": [
        "X = df.copy(deep=True)\n",
        "y = df['Target']\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002febf7",
      "metadata": {
        "id": "002febf7"
      },
      "source": [
        "### Training, testing and validation\n",
        "\n",
        "When developing a machine learning model, it is common practice to divide the available data into three subsets - training, validation, and testing. This is done to properly assess model performance and generalizability.\n",
        "\n",
        "The training set is used to fit the parameters of the model. The majority of the data, typically 60-80%, is allocated for training so that the model can learn the underlying patterns.\n",
        "\n",
        "The validation set is used to tune any hyperparameters of the model and make architectural choices. For example, it helps decide the number of hidden layers and units in a neural network. Typically 20% of data is used for validation.\n",
        "\n",
        "The test set provides an unbiased evaluation of the fully-trained model's performance. It is critical for getting an accurate estimate of how the model will work on new unseen data. Usually 20% of the data is reserved for testing.\n",
        "\n",
        "The splits should be made randomly while ensuring the class distribution is approximately balanced across all sets. No data from the validation or test sets should be used during training. This prevents overfitting and ensures the evaluation reflects real-world performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd21c10-5f73-4512-b31a-28ad694fd09d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ebd21c10-5f73-4512-b31a-28ad694fd09d",
        "outputId": "93f5db46-624d-4e97-ec53-c2094fc3f317"
      },
      "outputs": [],
      "source": [
        "# View data before splitting to training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08ebbfb",
      "metadata": {
        "id": "b08ebbfb"
      },
      "source": [
        "We are going to use the `train_test_split` function twice to split the data into training + validation and testing. Then, we split the `training + validation` into training and validation. We retrain the target values for now, so we can check how good is our prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff8049c-9c39-4880-9621-7f50e6c6062e",
      "metadata": {
        "id": "eff8049c-9c39-4880-9621-7f50e6c6062e"
      },
      "outputs": [],
      "source": [
        "# Specify split\n",
        "X_train_target, X_val_test_target, y_train, y_val_test = train_test_split(X, y, test_size=)\n",
        "X_test_target, X_val_target, y_test, y_val = train_test_split(X_val_test_target, y_val_test, test_size=)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8786fc9-6eca-4494-b2b6-5888abafee81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "b8786fc9-6eca-4494-b2b6-5888abafee81",
        "outputId": "0e8587b4-1585-4068-e63b-1a57edd8f209"
      },
      "outputs": [],
      "source": [
        "X_val_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819fd8c4-11f9-4b65-98dc-69ce5523220a",
      "metadata": {
        "id": "819fd8c4-11f9-4b65-98dc-69ce5523220a"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_target.drop(['Target'], axis=1)\n",
        "X_test = X_test_target.drop(['Target'], axis=1)\n",
        "X_val = X_val_target.drop(['Target'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34e84af-5f70-4c0a-ac58-cb28d5210429",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d34e84af-5f70-4c0a-ac58-cb28d5210429",
        "outputId": "29b65a2a-1930-43d9-9994-943ae65dd62a"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc159d1-a924-4d19-9b8c-8e47c9a09ed4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc159d1-a924-4d19-9b8c-8e47c9a09ed4",
        "outputId": "313e8423-4c85-430e-b29c-c46120d50acf"
      },
      "outputs": [],
      "source": [
        "y_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c4b8bdd-ae18-4748-af53-893cb930dbd0",
      "metadata": {
        "id": "6c4b8bdd-ae18-4748-af53-893cb930dbd0"
      },
      "source": [
        "# Decision tree classifier\n",
        "\n",
        "## Classification: Decision Trees\n",
        "\n",
        "Classification is the task of predicting a categorical target variable based on input data. There are two main types of classification:\n",
        "\n",
        "**Binary classification**: The target variable has two possible classes, often labeled 0 and 1. The goal is to predict which of the two classes an input belongs to. Examples include spam detection, disease diagnosis, etc.\n",
        "\n",
        "**Multi-class classification**: The target variable has more than two possible discrete values or classes. The goal is to predict the specific class an input belongs to out of the multiple choices. Examples include image recognition, document categorization, etc.\n",
        "\n",
        "A decision tree is a flowchart-like structure where each internal node represents a test on a feature (e.g., \"is feature A > 5?\"), each branch represents an outcome of the test, and each leaf node represents a class label. A decision tree is a supervised learning model used for both binary and multi-class classification. It works by recursively partitioning the input space into smaller subspaces based on the value of different predictor variables. The goal is to create leaf nodes that contain cases with similar values of the target variable.\n",
        "\n",
        "The process of learning a decision tree involves selecting features and split points that best separate the classes, based on a criterion such as information gain or Gini impurity. It continues until a stopping criterion is met, like reaching a maximum depth or a minimum number of samples per leaf.\n",
        "\n",
        "The structure of a decision tree consists of:\n",
        "\n",
        "  *  __Root node__: This is the topmost node in the tree and contains the full dataset.\n",
        "  *  __Internal nodes__: These represent points where the data is split based on values of predictor variables.\n",
        "  *  __Branches__: These connect the internal nodesbased on possible values of predictors.\n",
        "  *  __Leaf nodes__: These represent the final classifications or predictions.\n",
        "\n",
        "Decision trees can handle both categorical and continuous predictors.\n",
        "\n",
        "Some key advantages of decision trees are:\n",
        "\n",
        "* __Interpretability__ - The tree structure and rules are easy to understand.\n",
        "* __Non-parametric__ - No assumptions about data distribution.\n",
        "* __Handles nonlinear relationships__ - By partitioning data recursively.\n",
        "* __Handles categorical variables__ - No need for dummy coding.\n",
        "\n",
        "![liquefaction decision tree](liq-dt.png)\n",
        "\n",
        "### GINI Impurity\n",
        "\n",
        "Gini impurity is used to evaluate how good a split is by calculating the impurity of the subsets resulting from the split. A lower Gini score means that the split is separating the classes well. It quantifies the disorder or uncertainty within a set of data.\n",
        "\n",
        "The Gini impurity for a binary classification is calculated as:\n",
        "$$Gini(t)=1âˆ’\\sum(p_i)^2$$\n",
        "\n",
        "where $p_i$â€‹ is the probability of class $i$ in the set.\n",
        "\n",
        "### Decision Tree splitting\n",
        "\n",
        "A decision tree decides a split by selecting a feature and a value to divide the dataset into two or more homogenous subsets, according to a certain criterion. The ultimate goal is to find the splits that produce the purest subsets, meaning that each subset ideally contains data points from only one class. Here's how it works:\n",
        "\n",
        "  - **Selection of Criteria**: The method used to decide a split depends on the criterion being used. Common criteria for classification tasks include Gini Impurity, Information Gain, and Chi-Squared. For regression tasks, variance reduction is often used.\n",
        "\n",
        "  - **Evaluate Each Feature and Potential Split**: For each feature in the dataset, the algorithm calculates the criterion's value for every potential split point. Split points can be the actual values of a continuous feature or different categories of a categorical feature.\n",
        "\n",
        "  - **Choose the Best Split**: The algorithm selects the feature and split point that produces the subsets with the highest purity according to the chosen criterion. For example:\n",
        "        In the case of Gini Impurity, the best split minimizes the impurity.\n",
        "        In the case of Information Gain, the best split maximizes the gain.\n",
        "\n",
        "  - **Create Subsets**: Once the best split has been identified, the dataset is divided into subsets according to the chosen feature and split point.\n",
        "\n",
        "  - **Repeat**: Steps 1-4 are repeated recursively on each of the subsets until a stopping criterion is met, such as reaching a certain tree depth or the subsets being pure enough.\n",
        "\n",
        "#### Example Using Gini Impurity\n",
        "\n",
        "  - For each feature, consider all possible values for splitting.\n",
        "  - Choose the split that results in the lowest weighted Gini Impurity.\n",
        "  - Calculate the Gini Impurity for each possible split as:\n",
        "  - Divide the dataset accordingly and continue the process.\n",
        "\n",
        "In the context of decision trees, the splitting process is essential as it helps the model generalize the pattern from the training data, enabling accurate predictions or classifications for unseen data. It's the core step in building the tree, and different algorithms might have variations in the splitting procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba21832",
      "metadata": {
        "id": "7ba21832"
      },
      "source": [
        "## Decision tree with SciKit Learn\n",
        "\n",
        "Scikit-learn, often referred to as sklearn, is one of the most popular libraries for machine learning in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6432b78c",
      "metadata": {
        "id": "6432b78c"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36c3350-9056-445f-8245-83a5572c2d11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "a36c3350-9056-445f-8245-83a5572c2d11",
        "outputId": "3b71182b-3182-4e18-d3e5-b1c81c1d1fca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddb57ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eddb57ef",
        "outputId": "a8d44617-536f-42d8-d7da-ef021f45722a"
      },
      "outputs": [],
      "source": [
        "print('Training score: %.2f%%' %(clf.score(X_train, y_train) * 100))\n",
        "print('Validation score: %.2f%%' %(clf.score(X_val, y_val) * 100))\n",
        "print('Testing score: %.2f%%' %(clf.score(X_test, y_test) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27cce1cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27cce1cd",
        "outputId": "223cfe02-8f25-4527-bf2d-bc62b8f347f9"
      },
      "outputs": [],
      "source": [
        "# View accuracy scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5d2be0",
      "metadata": {
        "id": "5f5d2be0"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3bf36ac-349e-43fc-85af-bc53a852d58f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bf36ac-349e-43fc-85af-bc53a852d58f",
        "outputId": "58f27a2c-238a-4984-8324-1f904fa5c983"
      },
      "outputs": [],
      "source": [
        "# Make a prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8d77a5",
      "metadata": {
        "id": "9a8d77a5"
      },
      "source": [
        "### Visualizing a decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb167b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2cb167b6",
        "outputId": "ae23a679-3446-462c-b2a5-bfc455a60986"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "tree.plot_tree(clf,\n",
        "               feature_names =['GWD (m)', 'L (km)', 'Slope (%)', 'PGA (g)'],\n",
        "               class_names=['Liquefaction', 'No liquefaction'],\n",
        "               filled = True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e5d64f",
      "metadata": {
        "id": "50e5d64f"
      },
      "source": [
        "### Update model\n",
        "\n",
        "We increase the `max_depth` to 7 so we have a better fit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf47a85",
      "metadata": {
        "id": "bcf47a85"
      },
      "source": [
        "> ðŸ’¡ Try varying the `max_depth` from 5 - 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970d310d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "970d310d",
        "outputId": "556ca01f-7ae7-4cea-eb79-223d800fe260"
      },
      "outputs": [],
      "source": [
        "# Specify depth\n",
        "clf = tree.DecisionTreeClassifier(max_depth= )\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b534b356",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b534b356",
        "outputId": "6b5a9dc9-7f38-4cdb-91d4-05457dcaa13c"
      },
      "outputs": [],
      "source": [
        "print('Training score: %.2f%%' %(clf.score(X_train, y_train) * 100))\n",
        "print('Validation score: %.2f%%' %(clf.score(X_val, y_val) * 100))\n",
        "print('Testing score: %.2f%%' %(clf.score(X_test, y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daf977b",
      "metadata": {
        "id": "3daf977b"
      },
      "source": [
        "### Summary of decision trees\n",
        "\n",
        "\n",
        "Pros and Cons\n",
        "- Pros: Simple to understand and visualize, able to handle categorical and numerical data.\n",
        "- Cons: Prone to overfitting, especially when the tree is deep, leading to poor generalization to unseen data. A small change in the data can lead to a very different tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9a713a-bb85-42ac-91ec-7eadf9d58c1b",
      "metadata": {
        "id": "2d9a713a-bb85-42ac-91ec-7eadf9d58c1b"
      },
      "source": [
        "# XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "XGBoost is a popular and efficient gradient boosting framework that's widely used in machine learning, including in scientific contexts. It builds on the idea of boosting, where weak learners are combined to create a strong learner.\n",
        "\n",
        "## Gradient Boosting Framework\n",
        "\n",
        "Gradient Boosting is an ensemble learning method that fits a sequence of weak learners (such as shallow decision trees) on modified versions of the data.\n",
        "\n",
        "The general form of the boosting model is:\n",
        "\n",
        "$$ f(x) = \\sum_{k=1}^K \\alpha_k h_k(x) $$\n",
        "\n",
        "Where:\n",
        "- $f(x)$ is the prediction for input $ x $\n",
        "- $ \\alpha_k $ is the weight of the $ k $-th weak learner\n",
        "- $ h_k(x) $ is the prediction of the $ k $-th weak learner\n",
        "- $ K $ is the total number of weak learners\n",
        "\n",
        "## Algorithm\n",
        "\n",
        "1. **Initialization:** Start with a constant prediction $ f_0(x) $\n",
        "2. **Iteratively Add Trees:** For $ k = 1 $ to $ K $:\n",
        "   a. Compute the negative gradient (or \"pseudo-residuals\") of the loss function:\n",
        "    $$ r_i = -\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} $$\n",
        "   b. Fit a weak learner to predict the pseudo-residuals.\n",
        "   c. Compute the weight $ \\alpha_k $ that minimizes the loss.\n",
        "   d. Update the model by adding the weighted weak learner:\n",
        "      $$ f_k(x) = f_{k-1}(x) + \\alpha_k h_k(x) $$\n",
        "\n",
        "## XGBoost Enhancements\n",
        "\n",
        "XGBoost adds several enhancements to the basic gradient boosting framework:\n",
        "\n",
        "### Regularization\n",
        "\n",
        "XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in the loss function to prevent overfitting:\n",
        "\n",
        "$$ \\text{Loss} = L(y, f(x)) + \\lambda_1 \\sum |\\alpha_k| + \\lambda_2 \\sum \\alpha_k^2 $$\n",
        "\n",
        "### Handling Missing Data\n",
        "\n",
        "XGBoost can automatically learn the best direction to handle missing values during training, allowing it to handle datasets with missing data.\n",
        "\n",
        "### Column Block and Parallelization\n",
        "\n",
        "XGBoost employs a column block structure that allows parallelization over both instances and features, leading to efficient computation.\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "- **learning_rate:** Step size shrinkage to prevent overfitting.\n",
        "- **max_depth:** Maximum depth of the decision trees.\n",
        "- **n_estimators:** Number of boosting rounds.\n",
        "- **subsample:** Fraction of the data to be used for each boosting round, enabling stochastic gradient boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a48986",
      "metadata": {
        "id": "57a48986"
      },
      "source": [
        "![XGB](xgb.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183c5819-f595-4e67-b9d4-95967a2bb610",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "183c5819-f595-4e67-b9d4-95967a2bb610",
        "outputId": "da427488-7acc-47c8-a7d2-5ea78352c655"
      },
      "outputs": [],
      "source": [
        "# Create and train XGB model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74a76fa-d29e-420c-94f2-1cb3d81fb96f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74a76fa-d29e-420c-94f2-1cb3d81fb96f",
        "outputId": "1b4a9d82-22f0-4dfe-de94-a1ccf5a5d21f"
      },
      "outputs": [],
      "source": [
        "print('Training score: %.2f%%' %(xgb.score(X_train, y_train) * 100))\n",
        "print('Validation score: %.2f%%' %(xgb.score(X_val, y_val) * 100))\n",
        "print('Testing score: %.2f%%' %(xgb.score(X_test, y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edbd3360-f0e8-48d2-bfcb-e1f1f41c68e2",
      "metadata": {
        "id": "edbd3360-f0e8-48d2-bfcb-e1f1f41c68e2"
      },
      "source": [
        "## Explainable AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6290d0",
      "metadata": {
        "id": "4f6290d0"
      },
      "source": [
        "### Feature Importance Metrics\n",
        "\n",
        "Feature importance metrics provide insights into the contribution of individual features (or variables) to a predictive model. Understanding the importance of different features can help in feature selection, model interpretation, and understanding the underlying relationships within the data.\n",
        "\n",
        "#### Types of Importance Metrics\n",
        "\n",
        "##### 1. **Weight-Based Importance**\n",
        "\n",
        "In models like linear regression or logistic regression, the magnitude of the coefficients can indicate the importance of features. For tree-based models like Random Forest, the importance of a feature can be calculated based on the average gain of the feature when it is used in trees.\n",
        "\n",
        "##### 2. **Permutation Importance**\n",
        "\n",
        "Permutation importance is a method that can be applied to any model. It is computed as follows:\n",
        "\n",
        "a. **Calculate a Performance Metric:** Train the model and calculate a performance metric (such as accuracy or MSE) using a validation set.\n",
        "b. **Permute the Feature Values:** Shuffle the values of the feature of interest across the validation set, destroying the relationship between the feature and the target.\n",
        "c. **Recompute the Performance Metric:** Use the permuted data to compute the performance metric again.\n",
        "d. **Calculate Importance:** The difference between the original metric and the permuted metric provides a measure of importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f74131",
      "metadata": {
        "id": "82f74131"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "## Compute feature importance for random forest\n",
        "def plot_feature_importance(X, model):\n",
        "    df2=pd.get_dummies(X)\n",
        "    features = df2.columns\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[-10:]  # top 10 features\n",
        "    plt.title('Feature Importances')\n",
        "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.show()\n",
        "\n",
        "    fi = pd.DataFrame({'feature': list(X.columns),\n",
        "                       'importance': model.feature_importances_}).\\\n",
        "                       sort_values('importance', ascending = False)\n",
        "\n",
        "    return fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4bd96a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "ba4bd96a",
        "outputId": "6d3e11bb-8e61-431a-884b-71465412b7ce"
      },
      "outputs": [],
      "source": [
        "# Display results for decision tree\n",
        "plot_feature_importance(X_test, clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5419885",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "f5419885",
        "outputId": "78769beb-5aca-4b57-92f7-0ee39d1fe254"
      },
      "outputs": [],
      "source": [
        "# Display results for XGB\n",
        "plot_feature_importance(X_test, xgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6675df68",
      "metadata": {
        "id": "6675df68"
      },
      "source": [
        "### 3. Shapley Values\n",
        "\n",
        "\n",
        "SHAP values are based on cooperative game theory and provide a unified measure of feature importance. They give the average contribution of a feature value to every possible prediction.\n",
        "\n",
        "For a model with $M$ features, the Shapley value for feature $ i $ is computed as:\n",
        "\n",
        "$$ \\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(M - |S| - 1)!}{M!} [v(S \\cup \\{i\\}) - v(S)] $$\n",
        "\n",
        "Where:\n",
        "- $ S $ is a subset of features excluding feature $ i $\n",
        "- $ N $ is the set of all features\n",
        "- $ v(S) $ is the value function that gives the prediction for subset $ S $"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d5abb9",
      "metadata": {
        "id": "17d5abb9"
      },
      "source": [
        "\n",
        "### Understanding SHAP Values: An Analogy\n",
        "\n",
        "SHAP values help us understand how different features (or variables) contribute to a prediction in a machine learning model. To explain this without mathematics, let's use an analogy of three team members working together on a project. Imagine that the team members are Alice, Bob, and Charlie, and we want to know how much each one contributed to the project's success.\n",
        "\n",
        "#### The Project\n",
        "\n",
        "The project's success is measured by the total output, and we want to fairly distribute the credit for this success among Alice, Bob, and Charlie.\n",
        "\n",
        "#### Shapley Value Concept\n",
        "\n",
        "Imagine breaking down the project into smaller tasks and observing how much the output changes when each team member is added or removed. The goal is to find the average contribution of each member over all possible ways the team could have been formed.\n",
        "\n",
        "##### Step by Step Explanation\n",
        "\n",
        "1. **No Team Members:** First, we measure the output with no team members working (a baseline).\n",
        "2. **Adding One Member:** Then, we add each team member one by one and measure how much the output changes.\n",
        "   - With Alice alone\n",
        "   - With Bob alone\n",
        "   - With Charlie alone\n",
        "3. **Adding Two Members:** Next, we measure the change in output with pairs of team members:\n",
        "   - Alice and Bob\n",
        "   - Alice and Charlie\n",
        "   - Bob and Charlie\n",
        "4. **All Three Members:** Finally, we measure the output with all three working together.\n",
        "\n",
        "#### Calculating Contributions\n",
        "\n",
        "By comparing all these different combinations, we can calculate the average contribution of each team member. The SHAP value for each person is their fair share of the contribution to the project's success.\n",
        "\n",
        "- **Alice's SHAP Value:** The average change in output when Alice is part of the team.\n",
        "- **Bob's SHAP Value:** The average change in output when Bob is part of the team.\n",
        "- **Charlie's SHAP Value:** The average change in output when Charlie is part of the team.\n",
        "\n",
        "![Shapley](shap.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11a487e-5955-43e9-b095-13c1688c3f82",
      "metadata": {
        "id": "b11a487e-5955-43e9-b095-13c1688c3f82"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "# Create explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06ef0e96-7e03-41c7-b84c-e7e76541d593",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "06ef0e96-7e03-41c7-b84c-e7e76541d593",
        "outputId": "c682d555-9c73-4b7e-b48b-dfd17d330ae2"
      },
      "outputs": [],
      "source": [
        "X_test_target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fbc8db8",
      "metadata": {
        "id": "2fbc8db8"
      },
      "source": [
        "SHAP values are a measure used to explain the output of machine learning models. They tell us how much each feature in the model contributes to a particular prediction, compared to a baseline prediction.\n",
        "\n",
        "#### Positive vs. Negative SHAP Values\n",
        "\n",
        "- **Positive SHAP Values:** When a feature has a positive SHAP value, it means that the presence (or value) of that feature pushes the model's output higher than the baseline.\n",
        "  - **Example:** In our lateral spreading prediction the PGA (a feature) gives a positive SHAP value, it means that higher PGA are generally associated with higher chance of lateral spreading.\n",
        "\n",
        "- **Negative SHAP Values:** Conversely, when a feature has a negative SHAP value, it means that the presence (or value) of that feature pushes the model's output lower than the baseline.\n",
        "  - **Example:** Still considering lateral spreading, if water table being too low (a feature) gives a negative SHAP value, it suggests that such sites are generally predicted to have less chance of lateral spreading.\n",
        "\n",
        "\n",
        "The term $E(f(x))$ represents the expected value or average prediction of the model over the entire dataset. Think of it as the \"baseline prediction.\" When you see $E(f(x))$ , it's referring to what the model would predict on average, without considering any specific feature values.\n",
        "\n",
        "- **Example:** In the context of predicting house prices, $E(f(x))$  might be the average chance of lateral spreading the model predicts when it doesn't consider any specific features of the sites. It's the starting point before we account for unique factors like having GWT or a high PGA."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb44a772",
      "metadata": {
        "id": "eb44a772"
      },
      "source": [
        "#### Positive prediction\n",
        "In this example, we show a site that is predicted to have positive lateral spreading (failure). Here, the high slope anlge (0.835%) has the most influence in the positive prediction of lateral spreading, followed by a shallow ground water depth of 1.8 m and its close proximity to the river (L = 0.74 km)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c775e64-96eb-458c-b88c-f3e8a8ae5269",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "7c775e64-96eb-458c-b88c-f3e8a8ae5269",
        "outputId": "73e99184-c529-452f-c1a0-838af2a62eae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "685e502f",
      "metadata": {
        "id": "685e502f"
      },
      "source": [
        "#### Negative prediction\n",
        "\n",
        "In this example, we show XGB predicting a site that has no chance of lateral spreading. The distance to the river (L ~ 2 km) seems to be the primary reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ac42b1-33b3-4e65-aad7-cb65f9ba4db3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "19ac42b1-33b3-4e65-aad7-cb65f9ba4db3",
        "outputId": "e8823612-3d7e-40f8-9cc6-61776669caad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dc6f0848",
      "metadata": {
        "id": "dc6f0848"
      },
      "source": [
        "### Global explanation\n",
        "\n",
        "We also observe global SHAP values and see how different features influence the results. High feature values are shown in red and Low feature values are shown in blue. For e.g., shorter distance to the river (blue points for $L$) are associated with high chance of lateral spreading. Similarly, deeper ground water depth (red GWD) means lower chances of lateral spreading.\n",
        "\n",
        "However, we do see that high PGA, shows less chance of lateral spreading, contradictory to the engineering understanding that high acceleration will cause more displacements and failures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd648353-fccb-4c88-98e2-8265e6d6f70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "dd648353-fccb-4c88-98e2-8265e6d6f70a",
        "outputId": "369a9174-7fd4-457d-b0af-7aaacfd0780d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "IMAGE_NAME": "taccsciapps/ds-nb-img:base-0.1.2",
    "UUID": "74a50b78-3e8a-11ed-8804-4effbeb9a4da",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
